# Adam: A Method for Stochastic Optimization 

---

## Общее.
Submitted on 22 Dec 2014 (v1), last revised 30 Jan 2017 (this version, v9)
Diederik P. Kingma, Jimmy Ba - что-то ещё есть известное?
первый чел основал openAI видимо?? 

читаю [здесь](https://arxiv.org/abs/1412.6980)

Основное внимание в этой статье уделяется оптимизации стохастических задач с пространствами параметров большой размерности. 
В этих случаях методы оптимизации более высокого порядка непригодны, и обсуждение в этой статье будет ограничено методами первого порядка.
который требует только градиентов первого порядка с минимальными требованиями к памяти
Наш метод разработан таким образом, чтобы сочетать преимущества двух недавно ставших популярными методов: AdaGrad (Duchi et al., 2011), который хорошо работает с разреженными градиентами, и RMSProp (Tieleman & Hinton, 2012), который хорошо работает в онлайновых и нестационарных условиях

если коротко, из чего вытек адам
изначально был GD потом SGD тут наверное пока сказать нечего, дальше пошла разумная мысль: давайте добавим "инерцию" - если двигались куда-то долго, то наверное ещё чуть чуть в эту сторону тоже надо бы. Так появился Momentum GD, а вместе с ним и Nesterov Accelerated Gradient (предлагается считать градиент в точке куда пришли после шага от момента, а не до)
