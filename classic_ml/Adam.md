# Adam: A Method for Stochastic Optimization 

---

Наработки и цитаты из статьи

## Общее.
Submitted on 22 Dec 2014 (v1), last revised 30 Jan 2017 (this version, v9)
Diederik P. Kingma, Jimmy Ba - что-то ещё есть известное?
первый чел основал openAI видимо?? также известные работы .. 

читаю [здесь](https://arxiv.org/abs/1412.6980)

Мотивация, цитаты из статьи
Основное внимание в этой статье уделяется оптимизации стохастических задач с пространствами параметров большой размерности. 
В этих случаях методы оптимизации более высокого порядка непригодны, и обсуждение в этой статье будет ограничено методами первого порядка.
который требует только градиентов первого порядка с минимальными требованиями к памяти
Наш метод разработан таким образом, чтобы сочетать преимущества двух недавно ставших популярными методов: AdaGrad (Duchi et al., 2011), который хорошо работает с разреженными градиентами, и RMSProp (Tieleman & Hinton, 2012), который хорошо работает в онлайновых и нестационарных условиях

если коротко, из чего вытек адам
изначально был GD потом SGD тут наверное пока сказать нечего, дальше пошла разумная мысль: давайте добавим "инерцию" - если двигались куда-то долго, то наверное ещё чуть чуть в эту сторону тоже надо бы. Так появился Momentum GD, а вместе с ним и Nesterov Accelerated Gradient (предлагается считать градиент в точке куда пришли после шага от момента, а не до - "заглядывание вперёд"). Дальше RMSProp Adagrad  (опиши в том же формате, по предложению) 

Дальше что предложили:
Все знают что предложили объединить два метода -.. .., но мало кто знает что ещё есть одно дополнительное улучшение - INITIALIZATION BIAS CORRECTION ... . Подробнее: формулы такие, это объединяет два метода. Зачем bias -.. . 
Почему забыли про Nesterov ? Не забыли, в той же статье предложено AdaMax - по сути просто добавление "заглядывания вперёд" от нестерова.

Результаты и дальнейшие работы:

---

Текст в тг

**Adam: A Method for Stochastic Optimization (ICLR 2015)**

Submitted on 22 Dec 2014 (v1), last revised 30 Jan 2017 (this version, v9), ArXiv  
Diederik P. Kingma (OpenAI, Google, работал над VAE, встретим в статьях в дальнейшем) и Jimmy Lei Ba (Университет Торонто, из популярного - работы над Layer Normalization)  

В самом начале авторы подчеркивают, что их метод рассчитан на ситуации, когда данных много, а сами градиенты могут быть «шумными» или разреженными. В таких случаях SGD с Momentum далеко не всегда работает оптимально, и возникает мысль об адаптивном подборе шага для каждого параметра (как было в AdaGrad) и об учёте предыдущих градиентов (как в RMSProp). Adam объединяет сильные стороны этих алгоритмов, делая процесс обучения более стабильным и «непривязанным» к слишком тщательному тюнингу гиперпараметров.  

Если коротко из чего вытек Adam: был классический GD, потом стохастический - SGD (mini-batch к примеру). Дальше добавили идею «инерции» (Momentum GD) – если мы двигались какое-то время в одном направлении, наверное стоит там же продолжить. За этим последовал Nesterov Accelerated Gradient, где оценивают градиент «заглянув вперёд», то есть после гипотетического шага, чтобы корректировать движение точнее (см картинка 2). Параллельно появились AdaGrad и RMSProp. AdaGrad делает шаги для тех параметров, что редко получают градиент, относительно больше (что очень полезно при разреженных данных). RMSProp ввёл сглаженную оценку дисперсии градиента, чтобы следить за «шумностью» обновлений.  

Описание предлагаемого метода:  
Adam рождается как логическое объединение идей AdaGrad и RMSProp - это знают все, как и то от чего произошло название (Adaptive Momentum). Однако мало кто упоминает, что важным новшеством стал механизм **Initialization Bias Correction**. О нём: из-за инициализации моментов нулями - оценки первых итераций оказывались заниженными, а значит градиенты недооценивались. Авторы устранили это, поделив накопленные моменты на 1-b1 и 1-b2 - гиперпараметры отвечающие за «эффект моментума» и сглаживание квадрата градиента. Эти гиперпараметры предлагается выбирать 0.9 и 0.999 соответственно, так что получается именно увеличение значения градиента. Формально в статье для доказательства сходимости используется факт уменьшения этих параметров со временем и стремление их к 0, но в реализациях в библиотеках никто не добавляет этот функционал, судя по всему из-за отсутствия существенных улучшений. Благодаря таким «подтянутым» моментам Adam не страдает от заниженных градиентов в начале обучения и ведёт себя более стабильно. Формулы метода представлены на картинке 1 (я в шоке что ты это читаешь, сможешь ответить зачем eps на последнем этапе?). Итого мы получаем:    
Адаптивный подбор шага: каждый параметр обновляется с учётом собственной «истории» градиентов, что особенно выгодно при разреженных данных (в духе AdaGrad) и при шумном обучении.  
Учёт «моментума»: благодаря компоненте m мы не теряем информацию о направленности векторов градиента, сглаживая колебания.  
Стабильность от сглаживания второго момента: снижает эффект резких скачков в градиенте и защищает от «взрывных» обновлений.  
И исправление смещения из-за начальной инициализации.  

Если Вы очень внимательный, то заметили, что забыта идея ускорееного метода Нестерова. Хотя в оригинальной статье и предлагается сразу метод AdaMax, он рассматривает лишь использование бесконечной нормы. В дальнейших работах для этого предложен Nadam, учитывающий соответствующее улучшение.

Эксперименты показали что всё работает, графики скучные, добавлю гифку визуализации сходимости.  

Из интересных фактов про работу:  
- Самая цитируемая работа в сфере ML. 210k+ на момент написания текста.
- Популярная константа Андрея Карпаты - 3e-4. Появилась в твите от 24 ноября 2016 года: "3e-4 — лучший показатель скорости обучения для Adam" следующий "(Я просто хотел, чтобы люди поняли, что это шутка...)"  
- Куча улучшений, к примеру AdamW (с разделением веса и регуляризации) и AMSGrad (с улучшенной сходимостью). 
