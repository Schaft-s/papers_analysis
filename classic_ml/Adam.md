# Adam: A Method for Stochastic Optimization 

---

## Общее.
Submitted on 22 Dec 2014 (v1), last revised 30 Jan 2017 (this version, v9)
Diederik P. Kingma, Jimmy Ba - что-то ещё есть известное?
первый чел основал openAI видимо?? 

читаю [здесь](https://arxiv.org/abs/1412.6980)

Основное внимание в этой статье уделяется оптимизации стохастических задач с пространствами параметров большой размерности. 
В этих случаях методы оптимизации более высокого порядка непригодны, и обсуждение в этой статье будет ограничено методами первого порядка.
который требует только градиентов первого порядка с минимальными требованиями к памяти
Наш метод разработан таким образом, чтобы сочетать преимущества двух недавно ставших популярными методов: AdaGrad (Duchi et al., 2011), который хорошо работает с разреженными градиентами, и RMSProp (Tieleman & Hinton, 2012), который хорошо работает в онлайновых и нестационарных условиях
